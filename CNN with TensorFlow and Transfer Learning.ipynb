{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# Import all the necessary files!\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom os import getcwd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TRANSFER LEARNING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing Inceptıon Model","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path_inception = \"../input/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n\n# Import the inception model  \nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\n# Create an instance of the inception model from the local pre-trained weights\nlocal_weights_file = path_inception\n\npre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n                                include_top = False, \n                                weights = None)\n\npre_trained_model.load_weights(local_weights_file)\n\n# Make all the layers in the pre-trained model non-trainable\nfor layer in pre_trained_model.layers:\n    layer.trainable = False\n    \n# Print the model summary\npre_trained_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we freeze the inception model by selecting mixed7 conv layer\nlast_layer = pre_trained_model.get_layer('mixed7') # Modelin Bu layerına kadar ki kısmını al demek.\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output\n\n# Expected Output:\n# ('last layer output shape: ', (None, 7, 7, 768))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Model building\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# The callback helps us to choose on which accuracy level we want to stop model   \nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('accuracy')>0.97):\n            print(\"\\nReached 97% accuracy so cancelling training!\")\n            self.model.stop_training = True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import RMSprop\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)             \n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a final sigmoid layer for classification\nx = layers.Dense  (6, activation='softmax')(x)          \n\nmodel = Model( pre_trained_model.input, x)  \n\nmodel.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'categorical_crossentropy', \n              metrics = ['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dot representation of the model\nfrom IPython.display import SVG\nimport tensorflow.keras.utils as Utils\nfrom keras.utils.vis_utils import model_to_dot\nSVG(model_to_dot(model).create(prog='dot', format='svg'))\nUtils.plot_model(model,to_file='model.png',show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We assign each image directory to different variables\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport os\nimport zipfile\nimport shutil\n\n# Define our example directories and files\ntrain_dir = '/kaggle/input/intel-image-classification/seg_train/seg_train/'\nvalidation_dir = '/kaggle/input/intel-image-classification/seg_test/seg_test/'\npred_dir = '/kaggle/input/intel-image-classification/seg_pred/seg_pred/'\n\n\n\ntrain_buildings_dir = os.path.join('/kaggle/input/intel-image-classification/seg_train/seg_train/buildings')\ntrain_forest_dir = os.path.join('/kaggle/input/intel-image-classification/seg_train/seg_train/forest')\ntrain_glacier_dir = os.path.join('/kaggle/input/intel-image-classification/seg_train/seg_train/glacier')\ntrain_mountain_dir = os.path.join('/kaggle/input/intel-image-classification/seg_train/seg_train/mountain')\ntrain_sea_dir = os.path.join('/kaggle/input/intel-image-classification/seg_train/seg_train/sea')\ntrain_street_dir = os.path.join('/kaggle/input/intel-image-classification/seg_train/seg_train/street')\n\n\nvalidation_buildings_dir = os.path.join('/kaggle/input/intel-image-classification/seg_test/seg_test/buildings')\nvalidation_forest_dir = os.path.join('/kaggle/input/intel-image-classification/seg_test/seg_test/forest')\nvalidation_glacier_dir = os.path.join('/kaggle/input/intel-image-classification/seg_test/seg_test/glacier')\nvalidation_mountain_dir = os.path.join('/kaggle/input/intel-image-classification/seg_test/seg_test/mountain')\nvalidation_sea_dir = os.path.join('/kaggle/input/intel-image-classification/seg_test/seg_test/sea')\nvalidation_street_dir = os.path.join('/kaggle/input/intel-image-classification/seg_test/seg_test/street')\n\n\npred_images_dir = os.path.join('/kaggle/input/intel-image-classification/seg_pred/seg_pred')\npred_images_fnames = os.listdir(pred_images_dir)\n\ntrain_buildings_fnames = os.listdir(train_buildings_dir)\ntrain_forest_fnames = os.listdir(train_forest_dir)\ntrain_glacier_fnames = os.listdir(train_glacier_dir)\ntrain_mountain_fnames = os.listdir(train_mountain_dir)\ntrain_sea_fnames = os.listdir(train_sea_dir)\ntrain_street_fnames = os.listdir(train_street_dir)\n\n\nvalidation_buildings_fnames = os.listdir(validation_buildings_dir)\nvalidation_forest_fnames = os.listdir(validation_forest_dir)\nvalidation_glacier_fnames = os.listdir(validation_glacier_dir)\nvalidation_mountain_fnames = os.listdir(validation_mountain_dir)\nvalidation_sea_fnames = os.listdir(validation_sea_dir)\nvalidation_street_fnames = os.listdir(validation_street_dir)\n\n\n\nprint(\"TRAİNİNG SET\")\nprint(\"train_buildings_fnames\",len(train_buildings_fnames))\nprint(\"train_forest_fnames\", len(train_forest_fnames))\nprint(\"train_glacier_fnames\", len(train_glacier_fnames))\nprint(\"train_mountain_fnames\", len(train_mountain_fnames))\nprint(\"train_sea_fnames\", len(train_sea_fnames))\nprint(\"train_street_fnames\", len(train_street_fnames))\n\n\nprint(\"*\" * 20)\nprint(\"VALIDATION SET\")\nprint(\"validation_buildings_fnames\",len(validation_buildings_fnames))\nprint(\"validation_forest_fnames\", len(validation_forest_fnames))\nprint(\"validation_glacier_fnames\", len(validation_glacier_fnames))\nprint(\"validation_mountain_fnames\", len(validation_mountain_fnames))\nprint(\"validation_sea_fnames\", len(validation_sea_fnames))\nprint(\"validation_street_fnames\", len(validation_street_fnames))\n\n\nprint(\"*\" * 20)\nprint(\"pred_images_fnames\", len(pred_images_fnames))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data augmentation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add our data-augmentation parameters to ImageDataGenerator\ntrain_datagen = ImageDataGenerator(\n      rescale = 1./255,\n      rotation_range=40,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      shear_range=0.2,\n      zoom_range=0.2,\n      horizontal_flip=True,\n      fill_mode='nearest')\n\n# Note that the validation data should not be augmented!\ntest_datagen = ImageDataGenerator( rescale = 1.0/255.)\n\ndesired_batch_size = 32\n\n# Flow training images in batches of 20 using train_datagen generator\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    batch_size =desired_batch_size,\n                                                    class_mode = 'categorical',\n                                                    target_size = (150, 150))  \n\n\n# Flow validation images in batches of 20 using test_datagen generator\nvalidation_generator =  test_datagen.flow_from_directory( validation_dir,\n                                                          batch_size  = desired_batch_size,\n                                                          class_mode  = 'categorical',\n                                                          shuffle=False,\n                                                          target_size = (150, 150))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It shows  classes in the train set\ntrain_generator.class_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It shows  classes in the test set\nvalidation_generator.class_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = myCallback()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run this and see how many epochs it should take before the callback\n# fires, and stops training at 97% accuracy\n\n\nhistory = model.fit(train_generator,\n                              validation_data = validation_generator,\n                              steps_per_epoch = len(train_generator)/desired_batch_size,\n                              epochs = 20,\n                              validation_steps = len(validation_generator)/desired_batch_size,\n                              verbose = 1,\n                              callbacks = [callbacks]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting accuracy and loss values of the train and validation sets\n%matplotlib inline\nimport matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction of Test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Test images\npred=model.evaluate_generator(validation_generator,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels= validation_generator.labels\ntest_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot confusion matrix \nimport matplotlib.pyplot as plt\n# Note: This code snippet for confusion-matrix is taken directly from the SKLEARN website.\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=30)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Actual class')\n    plt.xlabel('Predicted class')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics as metrics\nfrom collections import Counter\nimport itertools\n\nvalidation_generator.reset()\nY_pred = model.predict_generator(validation_generator)\ny_pred = np.argmax(Y_pred, axis=-1)\nclasses = validation_generator.classes\n\nprint(sum(y_pred==classes)/validation_generator.samples)\n\nconfusion_matrix = metrics.confusion_matrix(y_true=classes, y_pred=y_pred) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_matrix, classes = range(6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# UPLOAD YOUR OWN PHOTO AND RUN IT IF YOU WISH\n\n\nimport numpy as np\n\nfrom keras.preprocessing import image\n\n\n \n# predicting images\npath = \"../input/photos/a1.png\"\nimg = image.load_img(path, target_size=(150, 150))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\n\nimages = np.vstack([x])\nclasses = model.predict(images, batch_size=10)\n\nprint(classes)\n\n\n\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}